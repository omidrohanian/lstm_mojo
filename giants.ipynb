{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this program, we assemble a body of text extracted from semantically related articles in Wikipedia, and then run it through a POS tagger. We then proceed to use these POS tags as labels in a supervised sequence labeling task using a Long Short Term Memory Network (LSTM).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dedicated to the spirit of Mr Mojo Risin \n",
    "# Omid Rohanian\n",
    "\n",
    "import wikipedia, string, re\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn.preprocessing\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doors = ['The Doors', 'Jim Morrison', 'Ray Manzarek', 'Robby Krieger', 'John Densmore', 'Strange Days (album)',\n",
    "         'The Doors (album)', 'Absolutely Live (The Doors album)', 'L.A. Woman', 'Light My Fire', 'Hello, I Love You',\n",
    "         'Touch Me (The Doors song)', 'Waiting for the Sun', 'The Soft Parade', 'Morrison Hotel', \n",
    "         'Other Voices (The Doors album)', 'Full Circle (The Doors album)', 'An American Prayer', 'The Doors â€“ 30 Years Commemorative Edition',\n",
    "         'Alabama Song', 'The End (The Doors song)', 'Counterculture of the 1960s', 'Break On Through (To the Other Side)',\n",
    "         'Live at the Matrix 1967', 'People Are Strange', 'Roadhouse Blues', 'Riders on the Storm', 'Love Me Two Times']\n",
    "text = ''\n",
    "for door in doors:\n",
    "    text += wikipedia.page(door).content\n",
    "\n",
    "sents = text.split('\\n')\n",
    "    \n",
    "# Each sentence is a list of words \n",
    "def preprocess(sents):\n",
    "    sents = [sent.translate(str.maketrans('','', string.punctuation)).strip(string.digits).lower() for sent in sents]\n",
    "    sents = [word_tokenize(sent) for sent in sents]\n",
    "    return [[word for word in sent if word not in set(stopwords.words('english'))] for sent in sents]\n",
    "\n",
    "# We will tag all the words with unique POS tags and later use these as labels for our classification task\n",
    "\n",
    "tagger = UnigramTagger(brown.tagged_sents(categories='news'))\n",
    "sents = preprocess(sents)\n",
    "words = list(set([word for sent in sents for word in sent]))\n",
    "pos_tags = dict(tagger.tag(words))\n",
    "maxlengths=max([len(s) for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_dic={word:i+1 for (i,word) in enumerate(words)}\n",
    "words_tags_pairs=[tagger.tag(s) for s in sents]\n",
    "\n",
    "y=[[w[1] for w in wp] for wp in words_tags_pairs]\n",
    "X=[[words_dic[w[0]] for w in wp] for wp in words_tags_pairs]\n",
    "X=[[x+[0]*(maxlengths-len(x))] for x in X]\n",
    "\n",
    "all_tags=set([x for s in y for x in s])\n",
    "all_tags_dic={t:i for (t,i) in zip (all_tags,range(1,len(all_tags)+1))}\n",
    "all_tags_dic[\"eos\"]=0\n",
    "\n",
    "#One-hot encode the labels \n",
    "y_num=[[all_tags_dic[t] for t in s]+[0]*(maxlengths-len(s)) for s in y]\n",
    "label_binarizer = sklearn.preprocessing.LabelBinarizer()\n",
    "label_binarizer.fit(range(36))\n",
    "y_onehot=[label_binarizer.transform(s) for s in y_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preparing the train and test data to be fed into the network \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=seed)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "X_train=np.reshape(X_train,[len(X_train),maxlengths])\n",
    "X_test=np.reshape(X_test,[len(X_test),maxlengths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"time_distributed_1/Reshape_1:0\", shape=(?, 269, 36), dtype=float32)\n",
      "Train on 1596 samples, validate on 400 samples\n",
      "Epoch 1/30\n",
      "1596/1596 [==============================] - 5s - loss: 3.5350 - acc: 0.0034 - val_loss: 3.5086 - val_acc: 0.0036\n",
      "Epoch 2/30\n",
      "1596/1596 [==============================] - 4s - loss: 3.5031 - acc: 0.0490 - val_loss: 3.4652 - val_acc: 0.9579\n",
      "Epoch 3/30\n",
      "1596/1596 [==============================] - 4s - loss: 3.4600 - acc: 0.4079 - val_loss: 3.4203 - val_acc: 0.9632\n",
      "Epoch 4/30\n",
      "1596/1596 [==============================] - 4s - loss: 3.4155 - acc: 0.8032 - val_loss: 3.3727 - val_acc: 0.9656\n",
      "Epoch 5/30\n",
      "1596/1596 [==============================] - 4s - loss: 3.3684 - acc: 0.9275 - val_loss: 3.3213 - val_acc: 0.9661\n",
      "Epoch 6/30\n",
      "1596/1596 [==============================] - 4s - loss: 3.3177 - acc: 0.9531 - val_loss: 3.2648 - val_acc: 0.9664\n",
      "Epoch 7/30\n",
      "1596/1596 [==============================] - 4s - loss: 3.2619 - acc: 0.9588 - val_loss: 3.2015 - val_acc: 0.9664\n",
      "Epoch 8/30\n",
      "1596/1596 [==============================] - 4s - loss: 3.1991 - acc: 0.9605 - val_loss: 3.1289 - val_acc: 0.9664\n",
      "Epoch 9/30\n",
      "1596/1596 [==============================] - 4s - loss: 3.1279 - acc: 0.9610 - val_loss: 3.0449 - val_acc: 0.9664\n",
      "Epoch 10/30\n",
      "1596/1596 [==============================] - 4s - loss: 3.0452 - acc: 0.9611 - val_loss: 2.9443 - val_acc: 0.9664\n",
      "Epoch 11/30\n",
      "1596/1596 [==============================] - 4s - loss: 2.9470 - acc: 0.9612 - val_loss: 2.8230 - val_acc: 0.9664\n",
      "Epoch 12/30\n",
      "1596/1596 [==============================] - 4s - loss: 2.8286 - acc: 0.9612 - val_loss: 2.6754 - val_acc: 0.9664\n",
      "Epoch 13/30\n",
      "1596/1596 [==============================] - 4s - loss: 2.6851 - acc: 0.9612 - val_loss: 2.4949 - val_acc: 0.9664\n",
      "Epoch 14/30\n",
      "1596/1596 [==============================] - 4s - loss: 2.5094 - acc: 0.9611 - val_loss: 2.2755 - val_acc: 0.9663\n",
      "Epoch 15/30\n",
      "1596/1596 [==============================] - 4s - loss: 2.2980 - acc: 0.9612 - val_loss: 2.0195 - val_acc: 0.9663\n",
      "Epoch 16/30\n",
      "1596/1596 [==============================] - 4s - loss: 2.0528 - acc: 0.9612 - val_loss: 1.7339 - val_acc: 0.9663\n",
      "Epoch 17/30\n",
      "1596/1596 [==============================] - 4s - loss: 1.7803 - acc: 0.9612 - val_loss: 1.4435 - val_acc: 0.9663\n",
      "Epoch 18/30\n",
      "1596/1596 [==============================] - 4s - loss: 1.5060 - acc: 0.9612 - val_loss: 1.1715 - val_acc: 0.9663\n",
      "Epoch 19/30\n",
      "1596/1596 [==============================] - 4s - loss: 1.2509 - acc: 0.9612 - val_loss: 0.9390 - val_acc: 0.9663\n",
      "Epoch 20/30\n",
      "1596/1596 [==============================] - 4s - loss: 1.0302 - acc: 0.9612 - val_loss: 0.7569 - val_acc: 0.9663\n",
      "Epoch 21/30\n",
      "1596/1596 [==============================] - 4s - loss: 0.8585 - acc: 0.9612 - val_loss: 0.6240 - val_acc: 0.9663\n",
      "Epoch 22/30\n",
      "1596/1596 [==============================] - 4s - loss: 0.7303 - acc: 0.9612 - val_loss: 0.5302 - val_acc: 0.9663\n",
      "Epoch 23/30\n",
      "1596/1596 [==============================] - 5s - loss: 0.6388 - acc: 0.9612 - val_loss: 0.4631 - val_acc: 0.9663\n",
      "Epoch 24/30\n",
      "1596/1596 [==============================] - 4s - loss: 0.5712 - acc: 0.9612 - val_loss: 0.4133 - val_acc: 0.9663\n",
      "Epoch 25/30\n",
      "1596/1596 [==============================] - 4s - loss: 0.5192 - acc: 0.9612 - val_loss: 0.3749 - val_acc: 0.9663\n",
      "Epoch 26/30\n",
      "1596/1596 [==============================] - 4s - loss: 0.4781 - acc: 0.9612 - val_loss: 0.3444 - val_acc: 0.9663\n",
      "Epoch 27/30\n",
      "1596/1596 [==============================] - 4s - loss: 0.4432 - acc: 0.9612 - val_loss: 0.3199 - val_acc: 0.9663\n",
      "Epoch 28/30\n",
      "1596/1596 [==============================] - 4s - loss: 0.4154 - acc: 0.9612 - val_loss: 0.3000 - val_acc: 0.9663\n",
      "Epoch 29/30\n",
      "1596/1596 [==============================] - 4s - loss: 0.3908 - acc: 0.9612 - val_loss: 0.2837 - val_acc: 0.9664\n",
      "Epoch 30/30\n",
      "1596/1596 [==============================] - 4s - loss: 0.3696 - acc: 0.9612 - val_loss: 0.2704 - val_acc: 0.9664\n",
      "499/499 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.29182597994804382, 0.96111184358596802]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the LSTM network \n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import TimeDistributed,Input,Embedding,Masking\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "\n",
    "inputs=Input(shape=[maxlengths,])\n",
    "masked_inputs=Masking(mask_value=0)(inputs)\n",
    "embedding_words=Embedding(input_dim=len(words_dic)+1,output_dim=128)(masked_inputs)\n",
    "\n",
    "\n",
    "lstm=LSTM(units=50,return_sequences=True)(embedding_words)\n",
    "dropout=Dropout(0.5)(lstm)\n",
    "\n",
    "\n",
    "outputs=TimeDistributed(Dense(36,activation=\"softmax\"))(dropout)\n",
    "print(outputs)\n",
    "model=Model(inputs=inputs,outputs=outputs)\n",
    "\n",
    "#compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# fit the model on the training data\n",
    "model.fit(X_train,y_train,\n",
    "          batch_size=len(X_train),epochs=30,\n",
    "          validation_split=0.2)\n",
    "#evaluate the model on the test data (for faster training we have used the size of the whole data as batch size)\n",
    "model.evaluate(X_test,y_test,batch_size=len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
